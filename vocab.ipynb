{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Imports </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ebooklib\n",
    "import re\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.util import ngrams\n",
    "import gensim \n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading the epub file : </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epub2thtml(epub_path):\n",
    "    book = epub.read_epub(epub_path)\n",
    "    chapters = []\n",
    "    for item in book.get_items():\n",
    "        if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "            chapters.append(item.get_content())\n",
    "    return chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chap2text(chap):\n",
    "    output = ''\n",
    "    soup = BeautifulSoup(chap, 'html.parser')\n",
    "    text = soup.find_all(text=True)\n",
    "    for t in text:\n",
    "        if t.parent.name not in blacklist:\n",
    "            output += '{} '.format(t)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thtml2ttext(thtml):\n",
    "    Output = []\n",
    "    for html in thtml:\n",
    "        text =  chap2text(html)\n",
    "        Output.append(text)\n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epub2text(epub_path):\n",
    "    chapters = epub2thtml(epub_path)\n",
    "    ttext = thtml2ttext(chapters)\n",
    "    return ttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epub file as a list\n",
    "out=epub2text('The-Art-of-Digital-Marketing-The-Definitive-Guide-to-Creating-Strategic-Targeted-and-Measurable-Online-Campaigns.epub')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data Munging </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_string(text) :\n",
    "    return ' '.join(text)\n",
    "text = to_string(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the text in a newfile\n",
    "with open(\"text.cvs\", \"w\") as text_file:\n",
    "    print(f\"text: {text}\", file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading each line\n",
    "with open(\"text.cvs\", \"r\") as text:\n",
    "    text = text.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text) :\n",
    "    #removing newlines(\\n)\n",
    "    text = [l.strip('\\n') for l in lines]\n",
    "    text = [l.lower() for l in text]\n",
    "    text = [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "    if len(text) > 2:\n",
    "        return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = to_string(text)\n",
    "re.sub(\"[^A-Za-z']+\",' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#using spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = ''.join(text)\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time \n",
    "\n",
    "t = time()\n",
    "\n",
    "text = [cleaning(doc) for doc in nlp.pipe(text, batch_size=50, n_threads=-1)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "phrases = Phrases(text, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngrams\n",
    "sentences = bigram[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "from nltk.stem.snowball import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the text per sentence \n",
    "text = re.compile('[.!?] ').split(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing empty spaces \n",
    "text = re.sub(r\"\\s+\",\" \", text, flags = re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting stop words\n",
    "text = [token.text for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3grams\n",
    "def extract_ngrams(data, num):\n",
    "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "  \n",
    "extract_ngrams(text,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from spacy.lang.en import English\n",
    "\n",
    "#with open(\"skills.cvs\") as skills:\n",
    "    #skills.read()\n",
    "\n",
    "#nlp = English()\n",
    "#doc = nlp(doc)\n",
    "\n",
    "#from spacy.matcher import PhraseMatcher\n",
    "\n",
    "#matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "#[nlp(skills) for skill in skills]\n",
    "#patterns = list(nlp.pipe(skills))\n",
    "#matcher.add(\"skills\", None, *patterns)\n",
    "\n",
    "#matches = matcher(doc)\n",
    "#print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "    text,\n",
    "    size=150,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    iter=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
